const ProductionRedditScraper = require('./production-scraper');
const fs = require('fs');
const path = require('path');

async function runProductionScraping() {
    console.log('ğŸš€ GreendoorAI Production Reddit Marketing Intelligence System');
    console.log('================================================================\n');
    
    // Load configuration
    const config = loadConfiguration();
    
    // Initialize production scraper
    const scraper = new ProductionRedditScraper(config);
    
    try {
        // Display pre-run information
        displayPreRunInfo(config);
        
        // Confirm production run
        console.log('âš ï¸  PRODUCTION MODE - This will create comprehensive output files');
        console.log('ğŸ“Š Expected runtime: 5-10 minutes depending on network conditions');
        console.log('ğŸ’¾ Output location: ./data/ directory\n');
        
        // Start production scraping
        const startTime = Date.now();
        console.log('ğŸ¯ Starting production scraping...\n');
        
        const report = await scraper.runProductionScraping();
        
        const duration = Date.now() - startTime;
        
        // Display success summary
        displaySuccessSummary(report, duration);
        
        return report;
        
    } catch (error) {
        console.error('âŒ Production scraping failed:', error.message);
        console.log('\nğŸ”§ Troubleshooting steps:');
        console.log('1. Check internet connection');
        console.log('2. Verify disk space availability');
        console.log('3. Check log files in ./logs/ directory');
        console.log('4. Try running test mode first: npm run test');
        
        process.exit(1);
    }
}

function loadConfiguration() {
    const defaultConfig = {
        maxRetries: 3,
        retryDelay: 5000,
        requestDelay: 2000,
        enableLogging: true,
        enableBackup: true,
        enableRecovery: true,
        dataDirectory: './data',
        logDirectory: './logs',
        archiveDirectory: './archive',
        enableMetrics: true,
        enableValidation: true,
        minContentLength: 50
    };
    
    // Try to load custom configuration
    const configPath = './production-config.json';
    if (fs.existsSync(configPath)) {
        try {
            const customConfig = JSON.parse(fs.readFileSync(configPath, 'utf8'));
            console.log('ğŸ“ Loaded custom configuration from production-config.json');
            return { ...defaultConfig, ...customConfig };
        } catch (error) {
            console.log('âš ï¸  Failed to load custom config, using defaults');
        }
    }
    
    return defaultConfig;
}

function displayPreRunInfo(config) {
    console.log('ğŸ“‹ Production Configuration:');
    console.log(`   â€¢ Request delay: ${config.requestDelay}ms (respectful rate limiting)`);
    console.log(`   â€¢ Max retries: ${config.maxRetries} (error resilience)`);
    console.log(`   â€¢ Logging: ${config.enableLogging ? 'Enabled' : 'Disabled'}`);
    console.log(`   â€¢ Backup: ${config.enableBackup ? 'Enabled' : 'Disabled'}`);
    console.log(`   â€¢ Data directory: ${config.dataDirectory}`);
    console.log(`   â€¢ Minimum content length: ${config.minContentLength} characters\n`);
    
    console.log('ğŸ¯ Target Subreddits:');
    console.log('   â€¢ r/sales, r/techsales (core sales communities)');
    console.log('   â€¢ r/startups, r/entrepreneur (business insights)');
    console.log('   â€¢ r/SaaS, r/marketing (industry specific)');
    console.log('   â€¢ r/salesforce, r/hubspot (tool discussions)');
    console.log('   â€¢ r/productivity, r/remotework (efficiency topics)\n');
    
    console.log('ğŸ“ Output Files:');
    console.log('   â€¢ urgent-content-[timestamp].csv (immediate use - score 20+)');
    console.log('   â€¢ high-value-[timestamp].csv (strategic content - score 15+)');
    console.log('   â€¢ detailed-analysis-[timestamp].csv (comprehensive data)');
    console.log('   â€¢ executive-summary-[timestamp].csv (management overview)');
    console.log('   â€¢ complete-dataset-[timestamp].json (full data export)');
    console.log('   â€¢ metrics-[timestamp].json (performance analytics)\n');
}

function displaySuccessSummary(report, duration) {
    const durationMinutes = Math.round(duration / 60000);
    const durationSeconds = Math.round((duration % 60000) / 1000);
    
    console.log('\nğŸ‰ PRODUCTION SCRAPING COMPLETED SUCCESSFULLY!');
    console.log('='.repeat(50));
    
    console.log(`â±ï¸  Total Duration: ${durationMinutes}m ${durationSeconds}s`);
    console.log(`ğŸ“Š Posts Scraped: ${report.statistics.totalPosts}`);
    console.log(`âœ¨ Quality Posts: ${report.statistics.qualityPosts}`);
    console.log(`ğŸš¨ Urgent Posts: ${report.statistics.urgentPosts}`);
    console.log(`â­ High-Value Posts: ${report.statistics.highValuePosts}`);
    console.log(`ğŸ“ˆ Success Rate: ${report.statistics.successRate}`);
    
    console.log('\nğŸ“ Generated Files:');
    report.outputFiles.forEach(file => {
        const icon = getFileIcon(file.type);
        const description = getFileDescription(file.type);
        console.log(`${icon} ${path.basename(file.path)} (${file.count} items) - ${description}`);
    });
    
    console.log('\nğŸ¯ Immediate Recommendations:');
    report.recommendations.forEach(rec => {
        console.log(`   â€¢ ${rec}`);
    });
    
    console.log('\nğŸ“‹ Next Steps:');
    report.nextSteps.forEach((step, index) => {
        console.log(`   ${index + 1}. ${step}`);
    });
    
    console.log('\nğŸ’¡ Pro Tips:');
    console.log('   â€¢ Start with urgent-content CSV for immediate wins');
    console.log('   â€¢ Use high-value CSV for strategic content planning');
    console.log('   â€¢ Import CSVs to Google Sheets for team collaboration');
    console.log('   â€¢ Schedule weekly runs for ongoing market intelligence');
    console.log('   â€¢ Track which insights drive best marketing performance');
    
    console.log('\nğŸš€ Your Reddit marketing intelligence system is ready!');
    console.log('   Data location: ./data/');
    console.log('   Logs location: ./logs/');
    console.log('   Backups location: ./data/backups/');
}

function getFileIcon(type) {
    const icons = {
        urgent: 'ğŸš¨',
        'high-value': 'â­',
        detailed: 'ğŸ“„',
        summary: 'ğŸ“‹',
        json: 'ğŸ’¾',
        metrics: 'ğŸ“Š'
    };
    return icons[type] || 'ğŸ“';
}

function getFileDescription(type) {
    const descriptions = {
        urgent: 'Immediate marketing opportunities',
        'high-value': 'Strategic content and case studies',
        detailed: 'Comprehensive analysis with insights',
        summary: 'Executive overview and priorities',
        json: 'Complete dataset for analysis',
        metrics: 'Performance and quality analytics'
    };
    return descriptions[type] || 'Marketing intelligence data';
}

// Error handling for production
process.on('unhandledRejection', (reason, promise) => {
    console.error('âŒ Unhandled Promise Rejection:', reason);
    process.exit(1);
});

process.on('uncaughtException', (error) => {
    console.error('âŒ Uncaught Exception:', error);
    process.exit(1);
});

// Handle graceful shutdown
process.on('SIGINT', () => {
    console.log('\nâš ï¸  Production scraping interrupted by user');
    console.log('ğŸ“ Partial data may be available in ./data/backups/');
    process.exit(0);
});

if (require.main === module) {
    runProductionScraping();
}

module.exports = runProductionScraping;